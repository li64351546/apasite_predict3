{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f34f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "from bisect import bisect\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938bd536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_now():#return time\n",
    "    curr_time = datetime.datetime.now()\n",
    "    return curr_time.strftime(\"%c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04859ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert_wig_into_bp_coverage\n",
    "\n",
    "def Convert_wig_into_bp_coverage(extracted_coverage,extracted_3UTR_region,strand_info):\n",
    "    bp_coverage = np.zeros(extracted_3UTR_region[-1] - extracted_3UTR_region[0])\n",
    "    relative_start = extracted_3UTR_region[0]\n",
    "    for i in range(len(extracted_coverage)):\n",
    "    \n",
    "        curr_region_start = extracted_3UTR_region[i] - relative_start\n",
    "        curr_region_end = extracted_3UTR_region[i+1] - relative_start\n",
    "        bp_coverage[curr_region_start:curr_region_end] = extracted_coverage[i]\n",
    "    if strand_info == '-':\n",
    "        bp_coverage = bp_coverage[::-1]\n",
    "    \n",
    "    return bp_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24660912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfgfile(cfg_file):\n",
    "    '''Parse configure file\n",
    "    '''\n",
    "    Group1_Tophat_aligned_file=''\n",
    "    Group2_Tophat_aligned_file=''\n",
    "    output_directory=''\n",
    "    Annotated_3UTR_file=''\n",
    "    Output_result_file=''\n",
    "    Num_least_in_group1_local=''\n",
    "    Num_least_in_group2_local=''\n",
    "    Coverage_cutoff_local = ''\n",
    "    FDR_cutoff_local = ''\n",
    "    Fold_change_cutoff_local = ''\n",
    "    PDUI_cutoff_local = ''\n",
    "    \n",
    "    for line in open(cfg_file,'r'):\n",
    "        if line[0] == '\\n' or line[0] == '#':\n",
    "            comments = line;\n",
    "        else:\n",
    "            line = line.rstrip();\n",
    "            command = line.split('=');\n",
    "            if command[0] == 'Group1_Tophat_aligned_Wig':\n",
    "                Group1_Tophat_aligned_file = command[1].split(',');\n",
    "            if command[0] == 'Group2_Tophat_aligned_Wig':\n",
    "                Group2_Tophat_aligned_file = command[1].split(',');\n",
    "            if command[0] == 'Output_directory':\n",
    "                output_directory = command[1]\n",
    "                if output_directory[-1] != '/':\n",
    "                    output_directory += '/'\n",
    "            if command[0] == 'Annotated_3UTR':\n",
    "                Annotated_3UTR_file = command[1]\n",
    "            if command[0] == 'Output_result_file':\n",
    "                Output_result_file = command[1]\n",
    "            \n",
    "            ##Parameters\n",
    "            if command[0] == 'Num_least_in_group1':\n",
    "                Num_least_in_group1_local = command[1]\n",
    "            if command[0] == 'Num_least_in_group2':\n",
    "                Num_least_in_group2_local = command[1]\n",
    "            if command[0] == 'Coverage_cutoff':\n",
    "                Coverage_cutoff_local = command[1]\n",
    "            if command[0] == 'FDR_cutoff':\n",
    "                FDR_cutoff_local = command[1]\n",
    "            if command[0] == 'Fold_change_cutoff':\n",
    "                Fold_change_cutoff_local = command[1]\n",
    "            if command[0] == 'PDUI_cutoff':\n",
    "                PDUI_cutoff_local = command[1]\n",
    "            \n",
    "    \n",
    "    if Group1_Tophat_aligned_file=='':\n",
    "        print(\"No Tophat aligned BAM file for group 1!\", file=sys.stderr)\n",
    "        exit(1)\n",
    "    if Group2_Tophat_aligned_file=='':\n",
    "        print(\"No Tophat aligned BAM file for group 2!\", file=sys.stderr)\n",
    "        exit(1)\n",
    "    if output_directory=='':\n",
    "        print(\"No output directory!\", file=sys.stderr)\n",
    "        exit(1)\n",
    "    if Annotated_3UTR_file=='':\n",
    "        print(\"No annotated 3' UTR file!\", file=sys.stderr)\n",
    "        exit(1)\n",
    "    if Output_result_file=='':\n",
    "        print(\"No result file name!\", file=sys.stderr)\n",
    "        exit(1)\n",
    "    return Group1_Tophat_aligned_file,Group2_Tophat_aligned_file,output_directory,Annotated_3UTR_file,Output_result_file,Num_least_in_group1_local,Num_least_in_group2_local,Coverage_cutoff_local,FDR_cutoff_local,Fold_change_cutoff_local,PDUI_cutoff_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a37a607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Group1_Tophat_aligned_file,Group2_Tophat_aligned_file,output_directory,Annotated_3UTR_file,Output_result_file,Num_least_in_group1_local,Num_least_in_group2_local,Coverage_cutoff_local,FDR_cutoff_local,Fold_change_cutoff_local,PDUI_cutoff_local = parse_cfgfile('/home/li/桌面/PROJECT6/apasite_predict2/DAPARS/DATA/DaPars_test_data_configure.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DaPars_Filtering(input_file, num_samples,num_group1 ,output_file):\n",
    "    \n",
    "    output_write = open(output_file,'w')\n",
    "    num_line = 0\n",
    "    \n",
    "    result_dict = {}\n",
    "    All_P_values = []\n",
    "    Selected_events_id = []\n",
    "    All_mean_abundance = []\n",
    "    \n",
    "    for line in open(input_file,'r'):\n",
    "        if num_line > 0:\n",
    "            fields = line.strip('\\n').split('\\t')\n",
    "            group1_coverages = np.zeros(2)\n",
    "            group2_coverages = np.zeros(2)\n",
    "            num_group1_pass = 0\n",
    "            group1_PDUIs = 0\n",
    "            for i in range(num_group1):\n",
    "                curr_long = fields[4+i*3]\n",
    "                curr_short = fields[5+i*3]\n",
    "                if curr_long != 'NA':\n",
    "                    curr_long  = float(curr_long)\n",
    "                    curr_short = float(curr_short)\n",
    "                    if curr_long + curr_short >= Coverage_cutoff:\n",
    "                        group1_PDUIs = group1_PDUIs + float(fields[6+i*3])\n",
    "                        num_group1_pass += 1\n",
    "                        group1_coverages[0] = group1_coverages[0] + curr_long  \n",
    "                        group1_coverages[1] = group1_coverages[1] + curr_short\n",
    "                    else:\n",
    "                        fields[4+i*3] = 'NA'\n",
    "                        fields[5+i*3] = 'NA'\n",
    "                        fields[6+i*3] = 'NA'\n",
    "            \n",
    "            \n",
    "            num_group2_pass = 0\n",
    "            group2_PDUIs = 0\n",
    "            for i in range(num_samples - num_group1):\n",
    "                curr_long = fields[4+(i+num_group1)*3]\n",
    "                curr_short = fields[5+(i+num_group1)*3]\n",
    "                if curr_long != 'NA':\n",
    "                    curr_long  = float(curr_long)\n",
    "                    curr_short = float(curr_short)\n",
    "                    if curr_long + curr_short >= Coverage_cutoff:\n",
    "                        group2_PDUIs = group2_PDUIs + float(fields[6+(i+num_group1)*3])\n",
    "                        num_group2_pass += 1\n",
    "                        group2_coverages[0] = group2_coverages[0] + curr_long  \n",
    "                        group2_coverages[1] = group2_coverages[1] + curr_short\n",
    "                    else:\n",
    "                        fields[4+(i+num_group1)*3] = 'NA'\n",
    "                        fields[5+(i+num_group1)*3] = 'NA'\n",
    "                        fields[6+(i+num_group1)*3] = 'NA'\n",
    "            \n",
    "            \n",
    "            \n",
    "            if num_group1_pass >= Num_least_in_group1 and num_group2_pass >= Num_least_in_group2:\n",
    "                group_diff = group1_PDUIs/num_group1_pass - group2_PDUIs/num_group2_pass\n",
    "                Final_group_diff = str(round(group_diff,2))\n",
    "                \n",
    "                All_mean_abundance.append([group1_PDUIs/num_group1_pass, group2_PDUIs/num_group2_pass])\n",
    "                \n",
    "                fields[-1] = str(Final_group_diff)\n",
    "                ratio_val,P_val = sp.stats.fisher_exact([group1_coverages/num_group1_pass,group2_coverages/num_group2_pass])\n",
    "                \n",
    "                All_P_values.append(P_val)\n",
    "                Selected_events_id.append(fields[0])\n",
    "                #print P_val\n",
    "                #print ratio_val\n",
    "            else:\n",
    "                fields[-1] = 'NA'\n",
    "            \n",
    "            \n",
    "            result_dict[fields[0]] = fields\n",
    "                    \n",
    "        else:\n",
    "            first_line = line.strip('\\n').split('\\t')      \n",
    "            \n",
    "        num_line += 1\n",
    "    \n",
    "    \n",
    "    ##Filtering\n",
    "    All_P_values = np.array(All_P_values, dtype = 'float')\n",
    "    All_p_adjust = multipletests(pvals=All_P_values, alpha=0.05, method=\"fdr_bh\")\n",
    "    first_line.insert(-1,'Group_A_Mean_PDUI')\n",
    "    first_line.insert(-1,'Group_B_Mean_PDUI')\n",
    "    first_line.extend(['P_val','adjusted.P_val','Pass_Filter'])\n",
    "    output_write.writelines('\\t'.join(first_line)+'\\n')\n",
    "    for curr_event_id in result_dict:\n",
    "        mean_PDUI_group1 = 'NA'\n",
    "        mean_PDUI_group2 = 'NA'\n",
    "        curr_P_val = 'NA'\n",
    "        curr_FDR_val = 'NA'\n",
    "        Pass_filter = 'N'\n",
    "        curr_fields = result_dict[curr_event_id]\n",
    "        if curr_event_id in Selected_events_id:\n",
    "            sel_ind = Selected_events_id.index(curr_event_id)\n",
    "            curr_P_val = str(All_P_values[sel_ind])\n",
    "            curr_FDR_val = str(All_p_adjust[1][sel_ind])\n",
    "            \n",
    "            mean_PDUI_group1 = All_mean_abundance[sel_ind][0]\n",
    "            mean_PDUI_group2 = All_mean_abundance[sel_ind][1]\n",
    "            \n",
    "            \n",
    "            if float(curr_FDR_val) <= FDR_cutoff and abs(float(curr_fields[-1]))>=PDUI_cutoff and abs(math.log((mean_PDUI_group1+1e-5)/(mean_PDUI_group2+1e-5),2))>=Fold_change_cutoff:\n",
    "                Pass_filter = 'Y'\n",
    "        \n",
    "        curr_fields.insert(-1,str(mean_PDUI_group1))\n",
    "        curr_fields.insert(-1,str(mean_PDUI_group2))\n",
    "        curr_fields.append(curr_P_val)\n",
    "        curr_fields.append(curr_FDR_val)\n",
    "        curr_fields.append(Pass_filter)\n",
    "        \n",
    "        output_write.writelines('\\t'.join(curr_fields) +'\\n')\n",
    "        \n",
    "        \n",
    "    output_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "222d80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Target_Wig_files(All_Wig_files, UTR_Annotation_file):\n",
    "    UTR_events_dict = {}\n",
    "    All_Samples_Total_depth = []\n",
    "    for line in open(UTR_Annotation_file,'r'):\n",
    "        fields = line.strip('\\n').split('\\t')\n",
    "        curr_chr = fields[0]\n",
    "        region_start = int(float(fields[1]))\n",
    "        region_end   = int(float(fields[2]))\n",
    "        curr_strand  = fields[-1]\n",
    "        UTR_pos = \"%s:%s-%s\" % (curr_chr, region_start, region_end)\n",
    "        end_shift = int(round(abs(int(region_start) - int(region_end)) * 0.2))\n",
    "        if curr_strand == '+':\n",
    "            region_end = str(int(region_end) - end_shift)\n",
    "        else:\n",
    "            region_start = str(int(region_start) + end_shift)\n",
    "        region_start = int(region_start) + 1\n",
    "        region_end   = int(region_end) - 1\n",
    "        if region_start + 50 < region_end:\n",
    "            UTR_events_dict[fields[3]] = [fields[0],region_start,region_end,fields[-1],UTR_pos]\n",
    "\n",
    "    ##Load coverage for all samples\n",
    "    All_samples_extracted_3UTR_coverage_dict = {}\n",
    "    for curr_wig_file in All_Wig_files:\n",
    "        curr_sample_All_chroms_coverage_dict = {}\n",
    "        num_line = 0\n",
    "        cur_sample_total_depth = 0\n",
    "        for line in open(curr_wig_file,'r'):\n",
    "            if '#' not in line and line[0:3] == 'chr':\n",
    "                fields = line.strip('\\n').split('\\t')\n",
    "                chrom_name = fields[0]\n",
    "                region_start = int(float(fields[1]))\n",
    "                region_end = int(float(fields[2]))\n",
    "                cur_sample_total_depth += int(float(fields[-1])) * (region_end - region_start)\n",
    "                if chrom_name not in curr_sample_All_chroms_coverage_dict:\n",
    "                    curr_sample_All_chroms_coverage_dict[chrom_name] = [[0],[0]]\n",
    "                if region_start > curr_sample_All_chroms_coverage_dict[chrom_name][0][-1]:\n",
    "                    curr_sample_All_chroms_coverage_dict[chrom_name][0].append(region_start)\n",
    "                    curr_sample_All_chroms_coverage_dict[chrom_name][1].append(0)\n",
    "                curr_sample_All_chroms_coverage_dict[chrom_name][0].append(region_end)\n",
    "                curr_sample_All_chroms_coverage_dict[chrom_name][1].append(int(float(fields[-1])))\n",
    "            num_line += 1\n",
    "        curr_sample_All_chroms_coverage_dict[chrom_name][1].append(0)\n",
    "        All_Samples_Total_depth.append(cur_sample_total_depth)\n",
    "        for curr_3UTR_event_id in UTR_events_dict:\n",
    "            curr_3UTR_structure = UTR_events_dict[curr_3UTR_event_id]\n",
    "            curr_chr = curr_3UTR_structure[0]\n",
    "            if curr_chr in curr_sample_All_chroms_coverage_dict:\n",
    "                curr_chr_coverage = curr_sample_All_chroms_coverage_dict[curr_chr]\n",
    "                region_start = curr_3UTR_structure[1]\n",
    "                region_end = curr_3UTR_structure[2]\n",
    "                left_region_index = bisect(curr_chr_coverage[0],region_start)\n",
    "                right_region_index = bisect(curr_chr_coverage[0],region_end)\n",
    "\n",
    "                extracted_coverage = curr_chr_coverage[1][left_region_index:right_region_index+1]\n",
    "                extracted_3UTR_region = curr_chr_coverage[0][left_region_index:right_region_index]\n",
    "                extracted_3UTR_region.insert(0,region_start)\n",
    "                extracted_3UTR_region.append(region_end)\n",
    "                if curr_3UTR_event_id not in All_samples_extracted_3UTR_coverage_dict:\n",
    "                    All_samples_extracted_3UTR_coverage_dict[curr_3UTR_event_id] = []\n",
    "                All_samples_extracted_3UTR_coverage_dict[curr_3UTR_event_id].append([extracted_coverage,extracted_3UTR_region])\n",
    "    return All_samples_extracted_3UTR_coverage_dict,np.array(All_Samples_Total_depth),UTR_events_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f88f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def De_Novo_3UTR_Coverage_estimation_Genome_for_TCGA_multiple_samples(All_Samples_curr_3UTR_coverages, UTR_start, UTR_end,curr_strand,weight_for_second_coverage):\n",
    "    '''For UTR-APA new\n",
    "       Load one chromosome by chromosome\n",
    "       Just for TCGA data analysis. So no peak evenness checking\n",
    "       Jan-17-2013\n",
    "       2-28-2013\n",
    "    '''\n",
    "    coverage_threshold = 20\n",
    "    search_point_start     = 200\n",
    "    search_point_end       = int(abs((UTR_end - UTR_start))*0.1)\n",
    "    \n",
    "    num_samples = len(All_Samples_curr_3UTR_coverages)\n",
    "    ##read coverage\n",
    "    Region_Coverages = []\n",
    "    Region_mean_Coverages = []\n",
    "    Region_first_100_coverage_all_samples = []\n",
    "    for i in range(num_samples):\n",
    "        curr_Region_Coverage_raw = All_Samples_curr_3UTR_coverages[i]##strand is reversed in load\n",
    "        curr_Region_Coverage = curr_Region_Coverage_raw/weight_for_second_coverage[i]\n",
    "        Region_mean_Coverages.append(np.mean(curr_Region_Coverage_raw))\n",
    "        Region_Coverages.append(curr_Region_Coverage)\n",
    "        curr_first_100_coverage = np.mean(curr_Region_Coverage_raw[0:99])\n",
    "        Region_first_100_coverage_all_samples.append(curr_first_100_coverage)\n",
    "    if sum(np.array(Region_first_100_coverage_all_samples) >= coverage_threshold) >= num_samples and UTR_end - UTR_start >= 150:\n",
    "        if curr_strand == \"+\":\n",
    "            search_region = list(range(UTR_start+search_point_start, UTR_end-search_point_end+1))\n",
    "        else:\n",
    "            search_region = list(range(UTR_end - search_point_start, UTR_start+search_point_end-1, -1))\n",
    "        \n",
    "        search_region_start = search_point_start\n",
    "        search_region_end   = UTR_end - UTR_start - search_point_end\n",
    "        Mean_squared_error_list  = []\n",
    "        Estimated_3UTR_abundance_list = []\n",
    "        for curr_point in range(search_region_start, search_region_end+1):\n",
    "            curr_search_point = curr_point\n",
    "            All_samples_result = [[],[],[]]\n",
    "            for curr_sample_region_coverage in Region_Coverages:\n",
    "                Mean_Squared_error,Long_UTR_abun,Short_UTR_abun = Estimation_abundance(curr_sample_region_coverage, curr_search_point)\n",
    "                All_samples_result[0].append(Mean_Squared_error)\n",
    "                All_samples_result[1].append(Long_UTR_abun)\n",
    "                All_samples_result[2].append(Short_UTR_abun)\n",
    "            \n",
    "            Mean_Squared_error = np.mean(np.array(All_samples_result[0]))\n",
    "            Mean_squared_error_list.append(Mean_Squared_error)\n",
    "            Estimated_3UTR_abundance_list.append([All_samples_result[1],All_samples_result[2]])\n",
    "\n",
    "\n",
    "\n",
    "        if len(Mean_squared_error_list) > 0:\n",
    "            min_ele_index = Mean_squared_error_list.index(min(Mean_squared_error_list))\n",
    "            \n",
    "            select_mean_squared_error = Mean_squared_error_list[min_ele_index]\n",
    "            UTR_abundances = Estimated_3UTR_abundance_list[min_ele_index]\n",
    "            selcted_break_point = search_region[min_ele_index]\n",
    "            \n",
    "        else:\n",
    "            select_mean_squared_error = 'Na'\n",
    "            UTR_abundances = 'Na'\n",
    "            selcted_break_point = 'Na'\n",
    "        \n",
    "    else:\n",
    "        select_mean_squared_error = 'Na'\n",
    "        UTR_abundances = 'Na'\n",
    "        selcted_break_point = 'Na'\n",
    "    \n",
    "    return select_mean_squared_error,selcted_break_point,UTR_abundances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f6539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def De_Novo_3UTR_Identification_Loading_Target_Wig_for_TCGA_Multiple_Samples_Main(argv=None):\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        print(\"Please provide the configure file ...\")\n",
    "        exit(1)\n",
    "    cfg_file = sys.argv[1]\n",
    "    print(\"[%s] Start Analysis ...\" % time_now(), file=sys.stderr)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Annotated_3UTR_file='DATA/hg19_refseq_extracted_3UTR.bed'\n",
    "    # Group1_Tophat_aligned_Wig=DATA/Condition_A_chrX.wig\n",
    "    # Group2_Tophat_aligned_Wig=DATA/Condition_B_chrX.wig\n",
    "    # Output_directory=DaPars_Test_data/\n",
    "    # Output_result_file=DaPars_Test_data\n",
    "\n",
    "\n",
    "\n",
    "    num_group_1 = len(Group1_Tophat_aligned_file)\n",
    "    All_Sample_files = Group1_Tophat_aligned_file[:]\n",
    "    All_Sample_files.extend(Group2_Tophat_aligned_file)\n",
    "    \n",
    "    \n",
    "    global Num_least_in_group1\n",
    "    global Num_least_in_group2\n",
    "    global Coverage_cutoff\n",
    "    global FDR_cutoff\n",
    "    global Fold_change_cutoff\n",
    "    global PDUI_cutoff\n",
    "    \n",
    "    if Num_least_in_group1_local != '':\n",
    "        Num_least_in_group1 = float(Num_least_in_group1_local)\n",
    "    if Num_least_in_group2_local != '':\n",
    "        Num_least_in_group2 = float(Num_least_in_group2_local)\n",
    "    if Coverage_cutoff_local != '':\n",
    "        Coverage_cutoff = float(Coverage_cutoff_local)\n",
    "    if FDR_cutoff_local != '':\n",
    "        FDR_cutoff = float(FDR_cutoff_local)\n",
    "    if Fold_change_cutoff_local != '':\n",
    "        Fold_change_cutoff = float(Fold_change_cutoff_local)\n",
    "    if PDUI_cutoff_local != '':\n",
    "        PDUI_cutoff = float(PDUI_cutoff_local)\n",
    "    \n",
    "\n",
    "\n",
    "    #Parameters\n",
    "    # Num_least_in_group1=1\n",
    "    # Num_least_in_group2=1\n",
    "    # Coverage_cutoff=30\n",
    "    # FDR_cutoff=0.05\n",
    "    # PDUI_cutoff=0.5\n",
    "    # Fold_change_cutoff=0.59\n",
    "\n",
    "\n",
    "    ##Prepare output directory\n",
    "    d = os.path.dirname(output_directory)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    temp_dir = d+'/tmp/'\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    Output_all_prediction_file = output_directory+Output_result_file+'_result_temp.txt'\n",
    "    Output_result = open(Output_all_prediction_file, 'w')\n",
    "    \n",
    "    num_samples = len(All_Sample_files)\n",
    "    \n",
    "    ##Debug\n",
    "    print(\"[%s] Loading coverage ...\" % time_now(), file=sys.stderr)\n",
    "    All_samples_Target_3UTR_coverages, All_samples_sequencing_depths, UTR_events_dict = Load_Target_Wig_files(All_Sample_files, Annotated_3UTR_file)\n",
    "    All_sample_coverage_weights = All_samples_sequencing_depths/np.mean(All_samples_sequencing_depths)\n",
    "    print(\"[%s] Loading coverage finished ...\" % time_now(), file=sys.stderr)\n",
    "    ##Write the first line\n",
    "    first_line = ['Gene','fit_value','Predicted_Proximal_APA','Loci']\n",
    "    for i in range(num_group_1):\n",
    "        curr_long_exp = 'A_%s_long_exp' % str(i+1)\n",
    "        curr_short_exp = 'A_%s_short_exp' % str(i+1)\n",
    "        curr_ratio ='A_%s_PDUI' % str(i+1)\n",
    "        first_line.extend([curr_long_exp,curr_short_exp,curr_ratio])\n",
    "    for i in range(num_samples - num_group_1):\n",
    "        curr_long_exp = 'B_%s_long_exp' % str(i+1)\n",
    "        curr_short_exp = 'B_%s_short_exp' % str(i+1)\n",
    "        curr_ratio ='B_%s_PDUI' % str(i+1)\n",
    "        first_line.extend([curr_long_exp,curr_short_exp,curr_ratio])\n",
    "    first_line.append('PDUI_Group_diff')\n",
    "    \n",
    "    Output_result.writelines('\\t'.join(first_line) + '\\n')\n",
    "    \n",
    "    \n",
    "    for curr_3UTR_id in UTR_events_dict:\n",
    "        curr_3UTR_structure = UTR_events_dict[curr_3UTR_id]\n",
    "        region_start = curr_3UTR_structure[1]\n",
    "        region_end   = curr_3UTR_structure[2]\n",
    "        curr_strand  = curr_3UTR_structure[-2]\n",
    "        UTR_pos = curr_3UTR_structure[-1]\n",
    "        if curr_3UTR_id in All_samples_Target_3UTR_coverages:\n",
    "            curr_3UTR_coverage_wig = All_samples_Target_3UTR_coverages[curr_3UTR_id]\n",
    "            curr_3UTR_all_samples_bp_coverage = []\n",
    "            for curr_sample_curr_3UTR_coverage_wig in curr_3UTR_coverage_wig: \n",
    "                curr_3UTR_curr_sample_bp_coverage = Convert_wig_into_bp_coverage(curr_sample_curr_3UTR_coverage_wig[0],curr_sample_curr_3UTR_coverage_wig[1],curr_strand)\n",
    "                curr_3UTR_all_samples_bp_coverage.append(curr_3UTR_curr_sample_bp_coverage)\n",
    "            \n",
    "            select_mean_squared_error,selcted_break_point,UTR_abundances = De_Novo_3UTR_Coverage_estimation_Genome_for_TCGA_multiple_samples(curr_3UTR_all_samples_bp_coverage, region_start, region_end,curr_strand,All_sample_coverage_weights)\n",
    "            \n",
    "            \n",
    "            if str(select_mean_squared_error) != \"Na\":\n",
    "                Long_3UTR_exp_all = np.array(UTR_abundances[0])\n",
    "                Short_3UTR_exp_all = np.array(UTR_abundances[1])\n",
    "                num_non_zero = sum((Long_3UTR_exp_all + Short_3UTR_exp_all)>0)\n",
    "                if num_non_zero == num_samples:\n",
    "                    All_Long_inclusion_ratios = []\n",
    "                    line_write = [curr_3UTR_id, \"%.1f\" % select_mean_squared_error, str(selcted_break_point), UTR_pos]\n",
    "                    for i in range(num_samples):\n",
    "                        curr_sample_ratio = float(UTR_abundances[0][i])/(float(UTR_abundances[0][i]) + float(UTR_abundances[1][i]))##long 3'UTR percentage\n",
    "                        All_Long_inclusion_ratios.append(curr_sample_ratio)\n",
    "                        line_write.append(\"%.2f\" % UTR_abundances[0][i])\n",
    "                        line_write.append(\"%.2f\" % UTR_abundances[1][i])\n",
    "                        line_write.append(\"%.2f\" % curr_sample_ratio)\n",
    "                    \n",
    "                    Group1_IR = All_Long_inclusion_ratios[:num_group_1]\n",
    "                    Group2_IR = All_Long_inclusion_ratios[num_group_1:]\n",
    "                    inclusion_ratio_Group_diff = np.mean(np.array(Group1_IR)) - np.mean(np.array(Group2_IR))\n",
    "                    \n",
    "                    line_write.append(\"%.2f\" % inclusion_ratio_Group_diff)\n",
    "                    \n",
    "                    Output_result.writelines( '\\t'.join(line_write)+'\\n')\n",
    "        \n",
    "    Output_result.close()\n",
    "    \n",
    "    print(\"[%s] Filtering the result ...\" % time_now(), file=sys.stderr)\n",
    "    \n",
    "    Output_Motif_filtered_result_file = output_directory+Output_result_file+'_All_Prediction_Results.txt'\n",
    "    #UTR_APA_Result_filtering(Output_all_prediction_file,Genome_seq_fasta,Output_Motif_filtered_result_file)\n",
    "    \n",
    "    DaPars_Filtering(Output_all_prediction_file, num_samples,num_group_1 ,Output_Motif_filtered_result_file)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.remove(Output_all_prediction_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.rmdir(temp_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"[%s] Finished!\" % time_now(), file=sys.stderr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca2eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Mon Jul 28 11:16:33 2025] Start Analysis ...\n",
      "[Mon Jul 28 11:16:33 2025] Loading coverage ...\n",
      "[Mon Jul 28 11:17:07 2025] Loading coverage finished ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Estimation_abundance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDe_Novo_3UTR_Identification_Loading_Target_Wig_for_TCGA_Multiple_Samples_Main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 104\u001b[0m, in \u001b[0;36mDe_Novo_3UTR_Identification_Loading_Target_Wig_for_TCGA_Multiple_Samples_Main\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    101\u001b[0m     curr_3UTR_curr_sample_bp_coverage \u001b[38;5;241m=\u001b[39m Convert_wig_into_bp_coverage(curr_sample_curr_3UTR_coverage_wig[\u001b[38;5;241m0\u001b[39m],curr_sample_curr_3UTR_coverage_wig[\u001b[38;5;241m1\u001b[39m],curr_strand)\n\u001b[1;32m    102\u001b[0m     curr_3UTR_all_samples_bp_coverage\u001b[38;5;241m.\u001b[39mappend(curr_3UTR_curr_sample_bp_coverage)\n\u001b[0;32m--> 104\u001b[0m select_mean_squared_error,selcted_break_point,UTR_abundances \u001b[38;5;241m=\u001b[39m \u001b[43mDe_Novo_3UTR_Coverage_estimation_Genome_for_TCGA_multiple_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_3UTR_all_samples_bp_coverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurr_strand\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAll_sample_coverage_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(select_mean_squared_error) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m     Long_3UTR_exp_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(UTR_abundances[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mDe_Novo_3UTR_Coverage_estimation_Genome_for_TCGA_multiple_samples\u001b[0;34m(All_Samples_curr_3UTR_coverages, UTR_start, UTR_end, curr_strand, weight_for_second_coverage)\u001b[0m\n\u001b[1;32m     36\u001b[0m All_samples_result \u001b[38;5;241m=\u001b[39m [[],[],[]]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m curr_sample_region_coverage \u001b[38;5;129;01min\u001b[39;00m Region_Coverages:\n\u001b[0;32m---> 38\u001b[0m     Mean_Squared_error,Long_UTR_abun,Short_UTR_abun \u001b[38;5;241m=\u001b[39m \u001b[43mEstimation_abundance\u001b[49m(curr_sample_region_coverage, curr_search_point)\n\u001b[1;32m     39\u001b[0m     All_samples_result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(Mean_Squared_error)\n\u001b[1;32m     40\u001b[0m     All_samples_result[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(Long_UTR_abun)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Estimation_abundance' is not defined"
     ]
    }
   ],
   "source": [
    "De_Novo_3UTR_Identification_Loading_Target_Wig_for_TCGA_Multiple_Samples_Main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
